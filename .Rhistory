#DO WE WANT NGRAMS? IF SO USE THIS ONE
splitemNG <- function(sentence) {
sentence <- gsub("\\n", " ", sentence)
sentence <- tolower(gsub("[^A-Za-z0-9'# -]", "", sentence)) #what we're keeping
words <- unlist(strsplit(sentence, "[ \n]")) #actually make the split
twograms <- make.ngrams(words, ngram.size=2) #make 2-grams
# data frame of term counts
DFwords <- data.frame(table(words))
DFtg <- data.frame(table(twograms))
names(DFwords) <- c("term", "freq")
names(DFtg) <- c("term", "freq")
#combine into master data frame
Di <- rbind(DFwords, DFtg)
Di
}
TFIDFperword <- function(tj, Di, Corpus) {
Di <- splitem(Di)
M = length(Corpus) # the number of documents in the Corpus
nij = Di[Di[,1]==tj,2] # the number of times tj appears in Di
ni1k = sum(Di[,2]) # the number of words in Di
itoj = length(grep(tj, Corpus, ignore.case=T)) #the number of documents in our corpus that contain tj
wji = (nij/ni1k) * log(M/itoj)
wji
}
# try to think of a more efficient way than loading the whole corpus in
TFIDFperword("algorithm", DRaw[1], DRaw)
TFIDF <- function(Di, Corpus) {
Di <- splitem(Di)
Di$tfidf <- numeric(nrow(Di))
M = length(Corpus) # the number of documents in the Corpus
ni1k = sum(Di[,2]) # the number of words in Di
for (j in 1:nrow(Di)) {
tj = as.character(Di[j,1])
nij = Di[Di[,1]==tj,2] # the number of times tj appears in Di
itoj = length(grep(tj, Corpus, ignore.case=T)) #the number of documents in our corpus that contain tj
Di$tfidf[j] =(nij/ni1k) * log(M/itoj)
}
Di <- Di[order(Di$tfidf, decreasing=T),]
Di <- Di[Di$tfidf>0 & Di$tfidf<1,] ## NEED TO DEAL WITH THE INFINITES
Di
}
D <- list()
for (i in 1:length(DRaw)) {
D[[i]] <- TFIDF(DRaw[i], DRaw)
}
Dc <- VCorpus(VectorSource(DRaw))
Dc
Dc[[2]]
meta(Dc[[2]])
Dcsw <- tm_map(Dc, removeWords, stopwords("english"))
as.character(Dcsw[[2]])
Dcst <- tm_map(Dcsw, stemDocument)
as.character(Dcst[[2]])
tm_map(Dcsw, stemDocument)
as.character(Dcsw[[2]])
Dtdm <- TermDocumentMatrix(Dc)
View(Dtdm)
inspect(Ddtm[5:10, 740:743])
inspect(Dtdm[5:10, 740:743])
inspect(Dtdm[5:10, 40:43])
Dtdm
inspect(Dtdm[5:10, ])
inspect(Dtdm[5:30, ])
inspect(Dtdm[105:130, ])
View(inspect(Dtdm[105:130, ]))
naw <- inspect(Dtdm[105:130, ])
View(naw)
Dc
length(Dc)
Dtdm
df <- data.frame(Dtdm)
df <- data.frame(inspect(Dtdm))
View(df)
length(Dc*.2)
length(Dc)*.2)
length(Dc)*.2
data("crude")tdm <- TermDocumentMatrix(crude,                          control = list(removePunctuation = TRUE,                                         stopwords = TRUE))
tdm <- TermDocumentMatrix(Dc,
control = list(removePunctuation = TRUE,
stopwords = TRUE))
inspect(tdm[1:25,])
tdmnaw <- TermDocumentMatrix(Dc)
inspect(tdmnaw[1:25,])
inspect(tdmnaw[Terms=="and",])
inspect(tdmnaw[tdmnaw$Terms=="and",])
inspect(tdmnaw[tdmnaw$Terms=="[5].",])
View(inspect(tdmnaw))
View(inspect(tdm))
tdm <- TermDocumentMatrix(Dc,
control = list(removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
stemming=TRUE))
View(inspect(tdm))
tdm <- TermDocumentMatrix(Dc,
control = list(removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
stemming=TRUE))
tdm <- TermDocumentMatrix(Dc,
control = list(removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE))
View(tdm)
View(inspect(tdm))
Dtdm <- TermDocumentMatrix(Dc,
control = list(weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
View(inspect(Dtdm))
inspect(tdm[c("basketball", "algorithm", "text"), ])
inspect(Dtdm[c("basketball", "algorithm", "text"), ])
Dtdmb <- TermDocumentMatrix(Dc,
control = list(bounds = list(global = c(1, length(Dc)*.2)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
Dtdmb <- TermDocumentMatrix(Dc,
control = list(bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
inspect(Dtdmb[c("basketball", "algorithm", "text"), ])
inspect(Dtdmb[c("basketball", "algorithm"), ])
View(inspect(Dtdmb))
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
Dtdm3 <- TermDocumentMatrix(Dc,
control = list(bounds = list(global = c(length(Dc)*.5, Inf)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
inspect(Dtdm3)
Dtdm3 <- TermDocumentMatrix(Dc,
control = list(bounds = list(global = c(length(Dc)*.6, Inf)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
inspect(Dtdm3)
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE , stemming=TRUE this threw a weird error
))
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE , stemming=TRUE #this threw a weird error
))
data("crude")termFreq(crude[[14]])strsplit_space_tokenizer <- function(x)    unlist(strsplit(as.character(x), "[[:space:]]+"))ctrl <- list(tokenize = strsplit_space_tokenizer,             removePunctuation = list(preserve_intra_word_dashes = TRUE),             stopwords = c("reuter", "that"),             stemming = TRUE,             wordLengths = c(4, Inf))termFreq(crude[[14]], control = ctrl)
data("crude")
termFreq(crude[[14]])
strsplit_space_tokenizer <- function(x)
unlist(strsplit(as.character(x), "[[:space:]]+"))
ctrl <- list(tokenize = strsplit_space_tokenizer,
removePunctuation = list(preserve_intra_word_dashes = TRUE),
stopwords = c("reuter", "that"),
stemming = TRUE,
wordLengths = c(4, Inf))
termFreq(crude[[14]], control = ctrl)
install.packages("SnowballC")
data("crude")
termFreq(crude[[14]])
strsplit_space_tokenizer <- function(x)
unlist(strsplit(as.character(x), "[[:space:]]+"))
ctrl <- list(tokenize = strsplit_space_tokenizer,
removePunctuation = list(preserve_intra_word_dashes = TRUE),
stopwords = c("reuter", "that"),
stemming = TRUE,
wordLengths = c(4, Inf))
termFreq(crude[[14]], control = ctrl)
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE , stemming=TRUE #this threw a weird error
))
View(inspect(Dtdm))
library(tm)
getSources()
?XMLSource
?URISource
getwd()
setwd("/Users/Seth/Documents/Silverchair/A1 taxonomyproject/Capstone simulation")
memory.limit()
2^31-1
2^30
2^31
?strsplit
library(rvest)
args <- commandArgs(TRUE)
findWikiTerms <- function(term) {
#get wiki terms
terms <- unlist(strsplit(term, ","))
terms <- gsub(" ", "_", terms)
wikiTerms <- character(length(terms))
for (j in 1:length(terms)) {
url <- paste("https://en.wikipedia.org/wiki/", terms[j], sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
if(grepl("s$", terms[j])) {
url <- paste("https://en.wikipedia.org/wiki/", substr(terms[j], 1, nchar(terms[j]) - 1), sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
} else {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
}
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
}
winners <- paste(wikiTerms, collapse=",")
print(winners)
}
findWikiTerms(args[1])
term <- "CCD"
terms <- unlist(strsplit(term, ","))
terms
wikiTerms <- character(length(terms))
for (j in 1:length(terms)) {
url <- paste("https://en.wikipedia.org/wiki/", terms[j], sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
if(grepl("s$", terms[j])) {
url <- paste("https://en.wikipedia.org/wiki/", substr(terms[j], 1, nchar(terms[j]) - 1), sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
} else {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
}
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
}
wikiTerms
url
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
title
term <- "Airborne reconnaissance,CCD image sensors,Charge-coupled devices,Imaging systems,Line scan sensors"
findWikiTerms <- function(term) {
#get wiki terms
terms <- unlist(strsplit(term, ","))
terms <- gsub(" ", "_", terms)
wikiTerms <- character(length(terms))
for (j in 1:length(terms)) {
url <- paste("https://en.wikipedia.org/wiki/", terms[j], sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
if(grepl("s$", terms[j])) {
url <- paste("https://en.wikipedia.org/wiki/", substr(terms[j], 1, nchar(terms[j]) - 1), sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
} else {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
}
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
}
winners <- paste(wikiTerms, collapse=",")
print(winners)
}
findWikiTerms(term)
term
sin(1)
sin(0)
sin(90)
sin(1)
sin(0.1)
x <- seq(-1, 1, by=0.1)
x
plot(sin(x))
big <- (-90, 90)
big <- seq(-90, 90)
plot(sin(big))
x <- seq(-10, 10, by=0.1)
plot(sin(x))
plot(x, sin(x))
plot(x, sin(x), xlim=c(-2,3))
plot(x, sin(x), xlim=c(-5,5))
sin(3.14)
pi
sin(pi)
sin(-pi)
cos(pi)
plot(x, cos(x))
plot(x, sin(x), xlim=c(-5,5))
lines(x, cos(x))
cos(0)
sin(0)
cos(pi)
cos(2pi)
cos(2 * pi)
sin(90)
cos(90)
sin(pi/4)
sin(3*pi/4)
pig <- seq(-10,10) * pi / 4
pig
plot(sin(pig),seq(-10,10))
plot(seq(-10,10),sin(pig))
plot(seq(-10,10),sin(pig), type-"l")
plot(seq(-10,10),sin(pig), type="l")
plot(seq(-10,10),sin(pig))
sin(9*pi/4)
sin(11*pi/4)
sin(pi/2)
sin(2*pi)
shiny::runApp('Documents/DSI/Wiki Terms app/shiny2WC')
?exists
runApp('Documents/DSI/Wiki Terms app/shiny2WC')
?wordcloud
runApp('Documents/DSI/Wiki Terms app/shiny2WC')
1/1/60 - 1533
as.date(1/1/60) - 1533
vec1 <- c(1,2,3,4,5)
vec2 <- c(6,2,7,4,8)
vec1 * vec2
vec1 <- c(1,2,3,4,5)
vec2 <- c(6,2,7,4,8)
cosDist <- function(vec1, vec2) {
sum( vec1 * vec2 ) / ( sqrt(sum( vec1 * vec1 )) * sqrt(sum( vec2 * vec2 )) )
}
cosDist(vec1, vec2)
sum( vec1 * vec2 )
sqrt(sum( vec1 * vec1 ))
sqrt(sum( vec2 * vec2 ))
vec1 <- c(1,2,3,4,5)
vec2 <- c(6,2,7,4,8)
vec3 <- c(10,20,7,30,40)
vec4 <- seq(100,500, by=100)
cosDist(vec2, vec3)
cosDist(vec3, vec4)
cosDist(vec4, vec4)
cosDist(vec1, vec4)
vec4
vec5 <- c(100,200,300,400,600)
cosDist(vec4, vec5)
cosDist(vec1, vec5)
vec1r <- runif(5)
vec2r <- runif(5)
cosDist(vec1, vec1r)
cosDist(vec1r, vec2r)
vec3r <- runif(5, min=100, max=1000)
cosDist(vec1r, vec3r)
vec3r
vec1r <- runif(5)
vec2r <- runif(5)
vec3r <- runif(5, min=100, max=1000)
cosDist(vec1r, vec3r) # I totally don't get this
setwd("~/Documents/DSI/notes/2-SYS-6018/Case Study 1 - crime/SYS-6018-cs1")
require(gdata)
library(MASS)
library(dplyr)
require(ks)
#require(rgdal)
source("CrimeUtil.R")
setwd("~/Documents/DSI/notes/2-SYS-6018/Case Study 1 - crime/SYS-6018-cs1")
## IF YOU WANT TO RUN IT IN FULL, LEAVE THIS AS TRUE
## IF YOU JUST WANT TO LOAD THE storesCrimeCounts.csv AND THEN RUN THE REST, CHANGE TO FALSE
runStores <- FALSE
# LOAD THE DATA
crimes <- read.csv("Annual_Crime_Dataset_2015.csv", stringsAsFactors = F)
crimes <- filter(crimes, GO.X.Coordinate != "") # remove crimes that have no location data
####
getCrimes <- function(add, crimes) {
# if there's a comma in the address, strip off what's before the comma (shopping mall name, etc.)
if (grepl(",", add)) {
add <- gsub("^.+, ", "", add)
}
# split on blank space
addS <- unlist(strsplit(add, " "))
# find the addresses that match
winners <- sapply(crimes$GO.Location, addyPart, addS = addS, USE.NAMES = F)
# subset to only those addresses
addDF <- crimes[winners, ]
# print the count
print(paste(add, nrow(addDF)))
# return the subset
addDF
}
addyPart <- function(addToCheck, addS) {
# set the threshold for what percentage of the address has to match
thresh <- .6 * length(addS)
# initiate the count of matching words
count <- 0
# make both lower case
addS <- tolower(addS)
addToCheck <- tolower(addToCheck)
# store the house number (or if there isn't one, store FALSE)
if (grepl('[0-9]', addS[1])) {
num <- addS[1]
} else {
num <- FALSE
}
# for each word in the address, store the count of how many are in the address we're checking
for (i in 1:length(addS)){
if (grepl(paste('\\b',addS[i],'\\b',sep=''), addToCheck)) {
count <- count + 1
}
}
if (count >= thresh) {
# if we pass the threshold AND it matches the house number, return TRUE
if (num != FALSE & grepl(paste('\\b',num,'\\b',sep=''), addToCheck)) {
TRUE
} else {
FALSE
}
} else {
FALSE
}
}
##BEGIN Removing commas from various appropriate columns in econ dataframe ~Kevin Sun
# econ$AvgIncome <- as.numeric(gsub(",","",econ$AvgIncome))
# econ$MedIncome <- as.numeric(gsub(",","",econ$MedIncome))
# econ$IncomeInequality <- as.numeric(gsub(",","",econ$IncomeInequality))
# write.csv(econ, "socio_econ.csv")
###END Removing commas from various appropriate columns in econ dataframe ~Kevin Sun
if (runStores==TRUE) {
# load manually collected stores location data
stores <- read.csv("stores_info.csv", stringsAsFactors = F)
# make a list with a data frame for each store and all its crimes
scdf <- lapply(stores$address, getCrimes, crimes = crimes)
#make a data frame with counts for each address
storeCrimeCount <- data.frame(store = character(nrow(stores)),
address = character(nrow(stores)),
crimes = numeric(nrow(stores)), stringsAsFactors = F)
for (j in 1:nrow(stores)) {
storeCrimeCount[j, ] <- c(stores$store[j], stores$address[j], nrow(scdf[[j]]))
}
# look at the addresses with zero crimes
zeros <- storeCrimeCount[storeCrimeCount$crimes==0, ]
# what percentage of our data set is zero?
nrow(zeros) / nrow(stores)
# create data.frame with store info and total crimes count ## OR load it below from a .csv
stores <- cbind(stores, as.numeric(storeCrimeCount$crimes))
names(stores)[ncol(stores)] <- "crimes2015"
head(stores)
#write.csv(stores, "storeCrimeCount.csv", row.names=F )
} else {
stores <- read.csv("storeCrimeCount.csv", header=T, stringsAsFactors=F)
}
head(stores)
